from __future__ import annotations
import random
import time
from typing import TYPE_CHECKING
from collections import defaultdict
import pandas as pd
import numpy as np
from llama_index.core import PromptTemplate
from llama_index.llms.ollama import Ollama
from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset
# Import evaluation classes and utilities
from llama_index.core.evaluation import (
    CorrectnessEvaluator,
    SemanticSimilarityEvaluator,
    RelevancyEvaluator,
    FaithfulnessEvaluator,
    PairwiseComparisonEvaluator,
)
from llama_index.core.evaluation.eval_utils import (
    get_responses,
    get_results_df,
)
from llama_index.core.evaluation import BatchEvalRunner
from app.common import (
        get_llama,
        get_base_nodes,
        load_stored_index,
        get_service_context
    )

from app.log import get_logger

import asyncio
import nest_asyncio
nest_asyncio.apply()


if TYPE_CHECKING:
    from llama_index.core.base.llms.types import CompletionResponse
    from llama_index.core.base.response.schema import RESPONSE_TYPE

__all__ = ["evaluate"]
logger = get_logger("evaluate")

def display_eval_df(query: str, response: Response, eval_result: str) -> None:
    eval_df = pd.DataFrame(
        {
            "Query": query,
            "Response": str(response),
            "Source": (
                response.source_nodes[0].node.get_content()[:1000] + "..."
            ),
            "Evaluation Result": eval_result,
        },
        index=[0],
    )
    eval_df = eval_df.style.set_properties(
        **{
            "inline-size": "600px",
            "overflow-wrap": "break-word",
        },
        subset=["Response", "Source"]
    )
    print("eval_df",eval_df)

def faithfull_evaluvation(response):
    # define evaluator
    evaluator = FaithfulnessEvaluator(llm=get_llama())
    eval_result = evaluator.evaluate_response(response=response)
    print(str(eval_result.passing))
    return str(eval_result.passing)


def relevancy_evaluvation(query, response):
    # define evaluator
    evaluator = RelevancyEvaluator(llm=get_llama())
    eval_result = evaluator.evaluate_response(query=query, response=response)
    print(str(eval_result))

    return str(eval_result)

    
def evaluate(response: RESPONSE_TYPE) -> CompletionResponse:
    # sentence_nodes, base_nodes = get_base_nodes()
    sentence_index , base_index = load_stored_index()
    query_engine = sentence_index.as_query_engine()
    query = "Succinctly summarize what disturbes Victor's sleep?"
    faithfull_eval = faithfull_evaluvation(response)
    time.sleep(30)
    relevancy_eval = relevancy_evaluvation(query, response)

    # num_nodes_eval = 10
    # data_generator = DatasetGenerator.from_documents(sentence_nodes, llm=get_llama())

    # eval_questions = data_generator.generate_questions_from_nodes()
    # print("eval_questions", eval_questions)

    # llm_evaluator = RelevancyEvaluator(llm=get_llama())

    # response_vector = query_engine.query(eval_questions[1])
    # eval_result = llm_evaluator.evaluate_response(
    #     query=eval_questions[1], response=response_vector
    # )
    # display_eval_df(eval_questions[1], response_vector, eval_result)


#     # TASK: Given the answer generated by the LLM in the previous step, write an evaluation prompt to have the LLM check if the answer
#     # appropriately answered the question.
#     # - If the answer sufficiently answers the question have the LLM respond with "Yes" and if not then "No".

    return faithfull_eval
